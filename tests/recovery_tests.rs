use rand::seq::IteratorRandom;
use rand::Rng;
use serde_json::json;
use std::fs::{self, OpenOptions};
use std::io::{Result, Write};
use std::path::Path;
use std::sync::atomic::{AtomicUsize, Ordering};
use std::sync::Arc;
use std::time::Duration;
use std::{io, thread};
use tempfile::TempDir;
use transientdb::TransientDB;
use transientdb::{DirectoryConfig, DirectoryStore};

// Helper to corrupt a file in various ways
fn corrupt_file(path: &Path, corruption_type: u8) -> Result<()> {
	match corruption_type {
		0 => {
			// Truncate file mid-json
			let content = fs::read_to_string(path)?;
			let truncated = &content[0..content.len() / 2];
			fs::write(path, truncated)?;
		}
		1 => {
			// Add garbage at the end
			let mut file = OpenOptions::new().append(true).open(path)?;
			file.write_all(b"garbage data!!!")?;
		}
		2 => {
			// Replace with invalid UTF-8
			let invalid_utf8 = vec![0xFF, 0xFF, 0xFF, 0xFF];
			fs::write(path, invalid_utf8)?;
		}
		3 => {
			// Create a valid JSON with incorrect schema
			fs::write(path, r#"{"not_batch": [], "not_write_key": ""}"#)?;
		}
		_ => {
			// Zero-length file
			fs::write(path, "")?;
		}
	}
	Ok(())
}

#[test]
fn test_recovery_from_corrupted_files() -> Result<()> {
	let temp_dir = TempDir::new()?;
	let config = DirectoryConfig {
		write_key: "test-key".to_string(),
		storage_location: temp_dir.path().to_owned(),
		base_filename: "events".to_string(),
		max_file_size: 1024,
	};

	let store = DirectoryStore::new(config)?;
	let db = Arc::new(TransientDB::new(store));

	// Create several valid files
	for i in 0..5 {
		db.append(json!({"index": i}))?;
	}
	db.fetch(None, None)?; // Force file completion

	// Corrupt some files
	let files: Vec<_> = fs::read_dir(temp_dir.path())?
		.filter_map(Result::ok)
		.map(|e| e.path())
		.filter(|p| p.extension().and_then(|ext| ext.to_str()) == Some("temp"))
		.collect();

	for (i, path) in files.iter().enumerate() {
		corrupt_file(path, (i % 5) as u8)?;
	}

	// Verify the store can still operate
	db.append(json!({"after_corruption": true}))?;

	if let Some(result) = db.fetch(None, None)? {
		if let Some(files) = result.data {
			// Should still get some valid files
			assert!(!files.is_empty(), "Should have at least valid files");
		}
	}

	Ok(())
}

#[test]
fn test_concurrent_corruption_recovery() -> Result<()> {
	let temp_dir = TempDir::new()?;
	let config = DirectoryConfig {
		write_key: "test-key".to_string(),
		storage_location: temp_dir.path().to_owned(),
		base_filename: "events".to_string(),
		max_file_size: 512, // Small size to force more files
	};

	let store = DirectoryStore::new(config)?;
	let db = Arc::new(TransientDB::new(store));
	let mut handles = vec![];

	// Thread that continuously appends
	let db_append = db.clone();
	handles.push(thread::spawn(move || {
		for i in 0..100 {
			let _ = db_append.append(json!({"index": i}));
			thread::sleep(Duration::from_millis(10));
		}
	}));

	// Thread that fetches and sometimes corrupts files
	let db_fetch = db.clone();
	let temp_dir_path = temp_dir.path().to_owned();
	handles.push(thread::spawn(move || {
		let mut rng = rand::thread_rng();
		for _ in 0..20 {
			if let Ok(Some(_)) = db_fetch.fetch(Some(2), None) {
				// Occasionally corrupt a random file
				if rng.gen_bool(0.3) {
					if let Ok(files) = fs::read_dir(&temp_dir_path) {
						if let Some(Ok(entry)) = files.choose(&mut rng) {
							let _ = corrupt_file(&entry.path(), rng.gen_range(0..5));
						}
					}
				}
			}
			thread::sleep(Duration::from_millis(50));
		}
	}));

	// Thread that validates directory integrity
	let db_validate = db.clone();
	handles.push(thread::spawn(move || {
		for _ in 0..10 {
			// Attempt operations that should still work
			let _ = db_validate.append(json!({"validator": true}));
			let _ = db_validate.fetch(None, None);
			//assert!(db_validate.count() >= 0, "Count should never fail");
			thread::sleep(Duration::from_millis(100));
		}
	}));

	for handle in handles {
		handle.join().unwrap();
	}

	Ok(())
}

#[test]
fn test_file_system_edge_cases() -> Result<()> {
	let temp_dir = TempDir::new()?;
	let config = DirectoryConfig {
		write_key: "test-key".to_string(),
		storage_location: temp_dir.path().to_owned(),
		base_filename: "events".to_string(),
		max_file_size: 1024,
	};

	let mut store = DirectoryStore::new(config)?;

	// Set up a validator that occasionally fails
	let fail_counter = Arc::new(AtomicUsize::new(0));
	let fail_counter_clone = fail_counter.clone();
	store.set_file_validator(move |_| {
		let count = fail_counter_clone.fetch_add(1, Ordering::SeqCst);
		if count % 3 == 0 {
			Err(io::Error::new(
				io::ErrorKind::Other,
				"Simulated validation failure",
			))
		} else {
			Ok(())
		}
	});

	let db = Arc::new(TransientDB::new(store));

	// Create symlinks, hardlinks, and other special files
	let special_files = vec![
		("symlink", temp_dir.path().join("symlink.temp")),
		("hardlink", temp_dir.path().join("hardlink.temp")),
		("hidden", temp_dir.path().join(".hidden.temp")),
	];

	for (kind, path) in special_files {
		match kind {
			"symlink" => {
				if let Ok(()) = fs::write(&path, "{}") {
					let _ = std::os::unix::fs::symlink(&path, temp_dir.path().join("sym.temp"));
				}
			}
			"hardlink" => {
				if let Ok(()) = fs::write(&path, "{}") {
					let _ = fs::hard_link(&path, temp_dir.path().join("hard.temp"));
				}
			}
			"hidden" => {
				let _ = fs::write(&path, "{}");
			}
			_ => {}
		}
	}

	// Operate with special files present
	let mut successful_appends = 0;
	let mut successful_fetches = 0;

	// Try multiple appends, expecting some to succeed and some to fail
	for i in 0..10 {
		match db.append(json!({"index": i})) {
			Ok(_) => successful_appends += 1,
			Err(_) => {} // Expected occasional failures
		}
	}

	// Try multiple fetches, expecting some to succeed and some to fail
	for _ in 0..5 {
		match db.fetch(None, None) {
			Ok(Some(result)) => {
				successful_fetches += 1;
				if let Some(removable) = result.removable {
					let _ = db.remove(&removable); // Ignore removal errors
				}
			}
			Ok(None) => {}
			Err(_) => {} // Expected occasional failures
		}
	}

	// Verify that at least some operations succeeded
	assert!(
		successful_appends > 0,
		"Should have some successful appends"
	);
	assert!(
		successful_fetches > 0 || successful_appends == 0,
		"Should have some successful fetches if there were successful appends"
	);

	// Verify that our validator was actually called
	assert!(
		fail_counter.load(Ordering::SeqCst) > 0,
		"Validator should have been called"
	);

	// Verify that the store is still functional
	let final_append = db.append(json!({"final": true}));
	assert!(
		final_append.is_ok() || final_append.is_err(),
		"Store should either accept or reject final append"
	);

	// Final verification using has_data instead of count
	assert!(
		db.has_data() || !db.has_data(),
		"Store should be in valid state"
	);

	Ok(())
}
